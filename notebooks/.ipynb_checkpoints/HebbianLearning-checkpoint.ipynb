{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c8e4963-42a5-4394-95c2-f90ae578da8c",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 35px;\"> <b>Hebbian Learning</b></h1>\n",
    "Recent research [1] suggests that our brain doesn't operate based on a global update rule, as proposed with the gradient descent algorithm, but on a \"simple\" local update rule. Thus comes the urge to find new and biologically more accurate training mechanisms. \n",
    "<br><br>\n",
    "\n",
    "One approach, which I will implement in this notebook, is based on a postulate from Donald Hebb in his book **The Organization of Behavior**, realeased in 1949. The following paper, on which this notebook is based on, explores this approach in the area of Meta-Learning\n",
    "<br><br>\n",
    "<div style=\"height:250px\">\n",
    "    <img src=\"../assets/paper-preview.png\" width=\"150\", border=\"1px\" style=\"vertical-align: middle;float:left;\">\n",
    "    \n",
    "    <h3 style=\"padding: 66px;float:left\"><b>Meta-Learning through Hebbian Plasticity in Random Networks</b><br><i>Elias Najarro Sebatian Risi - IT University Copenhagen</i></h3>\n",
    "</div>\n",
    "<div>\n",
    "\n",
    "This notebook consists of a simple implementation of a multilayer perceptron model with the Hebbian weight update rule using PyTorch, a ES alogirithm need for training and a bunch of custom made graphics and texts explaining the mechanisms.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b221afa-390a-4236-b2cd-4b944f9de950",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.) **Reinforcement Learning vs Hebbian Learning**\n",
    "Unlike in classical reinforcement learning, our goal is not to learn a static weighted policy network, but a hebbian update rule, which adjusts our network based on the inputs at runtime.\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align:center\"><img src=\"../assets/rlvshl.png\"></div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a97dd5ac-3348-4435-8392-6d79051d43d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some of this code is inspired or straight up copied from the official implementation\n",
    "# https://github.com/enajx/HebbianMetaLearning\n",
    "# Star it. Now. >:-)\n",
    "\n",
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import gym\n",
    "import pybullet_envs\n",
    "\n",
    "from gym import wrappers as w\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "from typing import List, Any\n",
    "\n",
    "from numba import njit # I AM SPEEEEED (JIT-Compilation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82707009-155c-4a4f-80b6-1b908b2e7619",
   "metadata": {},
   "source": [
    "## 2.) **A simple MLP with the Hebbian Update Rule**\n",
    "As in RL, we start out by building a simple policy network. It is a fully connected Multi Layer Perceptron without bias.\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align:center\"><img src=\"../assets/hebbiannetwork.png\"></div>\n",
    "<br>\n",
    "\n",
    "The key differences is the way we update it's weights. Gradient descend, which is a global update rule and therefore not biologically accurate, is replaced by the hebbian update rule. \n",
    "\n",
    "$$\\Delta w_{ij} = \\eta_w \\cdot o_i o_j$$\n",
    "\n",
    "This rule updates the weights dynamically throughout an episode using some evolved term inspired by the hebbian update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ad8a479-ef61-4159-9372-6c70093d0d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HebbianNetwork(nn.Module):\n",
    "    \"A simple MLP without bias\"\n",
    "    def __init__(self, input_space, action_space):\n",
    "        super(HebbianNetwork, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(input_space, 128, bias=False)\n",
    "        self.l2 = nn.Linear(128, 64, bias=False)\n",
    "        self.l3 = nn.Linear(64, action_space, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        state = torch.as_tensor(x).float().detach()\n",
    "        \n",
    "        x1 = torch.tanh(self.l1(state))   \n",
    "        x2 = torch.tanh(self.l2(x1))\n",
    "        o = self.l3(x2)  \n",
    "         \n",
    "        return state, x1, x2, o\n",
    "    \n",
    "# NUMBA JIT-Compilation goes brrrrrrrrrrr\n",
    "@njit\n",
    "def hebbian_update_rule(heb_coeffs, weights1_2, weights2_3, weights3_4, o0, o1, o2, o3):\n",
    "       \n",
    "        heb_offset = 0\n",
    "        # Layer 1         \n",
    "        for i in range(weights1_2.shape[1]): \n",
    "            for j in range(weights1_2.shape[0]):  \n",
    "                idx = (weights1_2.shape[0]-1)*i + i + j\n",
    "                weights1_2[:,i][j] += heb_coeffs[idx][3] * ( heb_coeffs[idx][0] * o0[i] * o1[j]\n",
    "                                                           + heb_coeffs[idx][1] * o0[i] \n",
    "                                                           + heb_coeffs[idx][2]         * o1[j]  + heb_coeffs[idx][4]) + heb_coeffs[idx][5]\n",
    "\n",
    "        heb_offset += weights1_2.shape[1] * weights1_2.shape[0]\n",
    "        # Layer 2\n",
    "        for i in range(weights2_3.shape[1]): \n",
    "            for j in range(weights2_3.shape[0]):  \n",
    "                idx = heb_offset + (weights2_3.shape[0]-1)*i + i+j\n",
    "                weights2_3[:,i][j] += heb_coeffs[idx][3] * ( heb_coeffs[idx][0] * o1[i] * o2[j]\n",
    "                                                           + heb_coeffs[idx][1] * o1[i] \n",
    "                                                           + heb_coeffs[idx][2]         * o2[j]  + heb_coeffs[idx][4]) + heb_coeffs[idx][5]\n",
    "    \n",
    "        heb_offset += weights2_3.shape[1] * weights2_3.shape[0]\n",
    "        # Layer 3\n",
    "        for i in range(weights3_4.shape[1]): \n",
    "            for j in range(weights3_4.shape[0]):  \n",
    "                idx = heb_offset + (weights3_4.shape[0]-1)*i + i+j \n",
    "                weights3_4[:,i][j] += heb_coeffs[idx][3] * ( heb_coeffs[idx][0] * o2[i] * o3[j]\n",
    "                                                           + heb_coeffs[idx][1] * o2[i] \n",
    "                                                           + heb_coeffs[idx][2]         * o3[j]  + heb_coeffs[idx][4]) + heb_coeffs[idx][5]\n",
    "\n",
    "        return weights1_2, weights2_3, weights3_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74790a42-9b81-4922-8b93-63e60dd25f09",
   "metadata": {},
   "source": [
    "Since the simplified hebbian update rule \n",
    "\n",
    "$$\\Delta w_{ij} = \\eta_w \\cdot o_i o_j$$\n",
    "\n",
    "cannot be used in supervised learning tasks due to only beeing a local update rule, we need to extend this rule in order to use it.\n",
    "\n",
    "The paper introduces four evolveable parameters for the hebbian update rule:\n",
    "\n",
    "- correlation term $A_w$\n",
    "- presynaptic term $B_w$\n",
    "- postsynaptic term $C_w$\n",
    "- bias $D_w$\n",
    "\n",
    "Which leads to a modified update rule, where the coefficients $A_w$, $B_w$, $C_w$ define the update dynamics of the network weights\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align:center\"><img src=\"../assets/hebbianrule.png\"></div>\n",
    "<br>\n",
    "\n",
    "These coefficients can be evolved using a basic ES algorithm which maximizes a cummulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff694e8-ce82-4c1c-b937-9c1292f889dd",
   "metadata": {},
   "source": [
    "## 3.) **Evolution strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de7b5c9e-5dbe-4d46-b630-b9cfa01a009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_hebb(environment : str, init_weights:str, evolved_parameters: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the policy network using some evolved parameters and environment.\n",
    "    \"\"\"\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            if init_weights == 'xa_uni':  \n",
    "                torch.nn.init.xavier_uniform(m.weight.data, 0.3)\n",
    "            elif init_weights == 'sparse':  \n",
    "                torch.nn.init.sparse_(m.weight.data, 0.8)\n",
    "            elif init_weights == 'uni':  \n",
    "                torch.nn.init.uniform_(m.weight.data, -0.1, 0.1)\n",
    "            elif init_weights == 'normal':  \n",
    "                torch.nn.init.normal_(m.weight.data, 0, 0.024)\n",
    "            elif init_weights == 'ka_uni':  \n",
    "                torch.nn.init.kaiming_uniform_(m.weight.data, 3)\n",
    "            elif init_weights == 'uni_big':\n",
    "                torch.nn.init.uniform_(m.weight.data, -1, 1)\n",
    "            elif init_weights == 'xa_uni_big':\n",
    "                torch.nn.init.xavier_uniform(m.weight.data)\n",
    "            elif init_weights == 'ones':\n",
    "                torch.nn.init.ones_(m.weight.data)\n",
    "            elif init_weights == 'zeros':\n",
    "                torch.nn.init.zeros_(m.weight.data)\n",
    "            elif init_weights == 'default':\n",
    "                pass\n",
    "            \n",
    "    # Unpack evolved parameters\n",
    "    hebb_coeffs = evolved_parameters\n",
    "\n",
    "    \n",
    "    # disable the autograd system\n",
    "    with torch.no_grad():\n",
    "                    \n",
    "        # Load environment\n",
    "        try:\n",
    "            env = gym.make(environment, verbose = 0)\n",
    "        except:\n",
    "            env = gym.make(environment)\n",
    "            \n",
    "        # env.render()  # render bullet envs\n",
    "\n",
    "        # get the input dimensions of the environment\n",
    "        input_dim = env.observation_space.shape[0]\n",
    "            \n",
    "        # Determine action space dimension\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        \n",
    "        # Initialize policy network\n",
    "        p = HebbianNetwork(input_dim, action_dim)          \n",
    "          \n",
    "        # Randomly sample initial weights from chosen distribution\n",
    "        p.apply(weights_init)\n",
    "        p = p.float()\n",
    "        \n",
    "        # Unpack network's weights\n",
    "        weights1_2, weights2_3, weights3_4 = list(p.parameters())\n",
    "            \n",
    "        # JIT\n",
    "        weights1_2 = weights1_2.detach().numpy()\n",
    "        weights2_3 = weights2_3.detach().numpy()\n",
    "        weights3_4 = weights3_4.detach().numpy()\n",
    "        \n",
    "        # reset the environment\n",
    "        observation = env.reset() \n",
    "\n",
    "        # Burnout phase for the bullet ant so it starts off from the floor\n",
    "        if environment == 'AntBulletEnv-v0':\n",
    "            action = np.zeros(8)\n",
    "            for _ in range(40):\n",
    "                __ = env.step(action)        \n",
    "        \n",
    "        # Normalize weights flag for non-bullet envs\n",
    "        normalised_weights = False if environment[-12:-6] == 'Bullet' else True\n",
    "\n",
    "\n",
    "        # Main loop\n",
    "        neg_count = 0 # count the amount of times we receive a negative reward\n",
    "        rew_ep = 0 # cummulative reward over an episode\n",
    "        t = 0 # timestep\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            # For obaservation ∈ gym.spaces.Discrete, we one-hot encode the observation\n",
    "            if isinstance(env.observation_space, Discrete): \n",
    "                observation = (observation == torch.arange(env.observation_space.n)).float()\n",
    "            \n",
    "            o0, o1, o2, o3 = p([observation])\n",
    "            \n",
    "            # JIT\n",
    "            o0 = o0.numpy()\n",
    "            o1 = o1.numpy()\n",
    "            o2 = o2.numpy()\n",
    "            \n",
    "            # preprocess the observation\n",
    "            if environment[-12:-6] == 'Bullet':\n",
    "                o3 = torch.tanh(o3).numpy()\n",
    "                action = o3\n",
    "            else: \n",
    "                if isinstance(env.action_space, Box):\n",
    "                    action = o3.numpy()                        \n",
    "                    action = np.clip(action, env.action_space.low, env.action_space.high)  \n",
    "                elif isinstance(env.action_space, Discrete):\n",
    "                    action = np.argmax(o3).numpy()\n",
    "                o3 = o3.numpy()\n",
    "\n",
    "            \n",
    "            # Environment simulation step\n",
    "            observation, reward, done, info = env.step(action)  \n",
    "            if environment == 'AntBulletEnv-v0': reward = env.unwrapped.rewards[1] # Distance walked\n",
    "            rew_ep += reward\n",
    "            \n",
    "            # env.render('human') # Gym envs\n",
    "                                       \n",
    "            # Early stopping conditions\n",
    "            if environment[-12:-6] == 'Bullet':\n",
    "                ## Special stopping condition for bullet envs\n",
    "                # always play 200 episodes\n",
    "                if t > 200:\n",
    "                    # after 200 episodes: count the amount of negative\n",
    "                    # reward we receive in a row\n",
    "                    neg_count = neg_count+1 if reward < 0.0 else 0\n",
    "                    \n",
    "                    # if we receive a negative reward 30 times in row, stop\n",
    "                    if (done or neg_count > 30):\n",
    "                        break\n",
    "            else:\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            t += 1\n",
    "            \n",
    "            #### Episodic/Intra-life hebbian update of the weights\n",
    "            weights1_2, weights2_3, weights3_4 = hebbian_update_rule(hebb_coeffs, weights1_2, weights2_3, weights3_4, o0, o1, o2, o3)\n",
    "            \n",
    "\n",
    "            # Normalise weights per layer\n",
    "            if normalised_weights == True:\n",
    "                (a, b, c) = (0, 1, 2) if not pixel_env else (2, 3, 4)\n",
    "                list(p.parameters())[a].data /= list(p.parameters())[a].__abs__().max()\n",
    "                list(p.parameters())[b].data /= list(p.parameters())[b].__abs__().max()\n",
    "                list(p.parameters())[c].data /= list(p.parameters())[c].__abs__().max()\n",
    "        \n",
    "        # close the environment\n",
    "        env.close()\n",
    "\n",
    "    return rew_ep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705511be-bf3d-49f0-a6f3-ee4d35c182d6",
   "metadata": {},
   "source": [
    "# References\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
