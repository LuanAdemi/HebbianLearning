{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wrapped-toilet",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 35px;\"> <b>Hebbian Learning</b></h1>\n",
    "Recent research [1] suggests that our brain doesn't operate based on a global update rule, as proposed with the gradient descent algorithm, but on a \"simple\" local update rule. Thus comes the urge to find new and biologically more accurate training mechanisms. \n",
    "<br><br>\n",
    "\n",
    "One approach, which I will implement in this notebook, is based on a postulate from Donald Hebb in his book **The Organization of Behavior**, realeased in 1949. The following paper, on which this notebook is based on, explores this approach in the area of Meta-Learning\n",
    "<br><br>\n",
    "<div style=\"height:250px\">\n",
    "    <img src=\"../assets/paper-preview.png\" width=\"150\", border=\"1px\" style=\"vertical-align: middle;float:left;\">\n",
    "    \n",
    "    <h3 style=\"padding: 66px;float:left\"><b>Meta-Learning through Hebbian Plasticity in Random Networks</b><br><i>Elias Najarro Sebatian Risi - IT University Copenhagen</i></h3>\n",
    "</div>\n",
    "<div>\n",
    "\n",
    "This notebook consists of a simple implementation of a multilayer perceptron model with the Hebbian weight update rule using PyTorch, a ES alogirithm need for training and a bunch of custom made graphics and texts explaining the mechanisms.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-promotion",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.) **Reinforcement Learning vs Hebbian Learning**\n",
    "Unlike in classical reinforcement learning, our goal is not to learn a static weighted policy network, but a hebbian update rule, which adjusts our network based on the inputs at runtime.\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align:center\"><img src=\"../assets/rlvshl.png\"></div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "played-actor",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# some of this code is inspired or straight up copied from the official implementation\n",
    "# https://github.com/enajx/HebbianMetaLearning\n",
    "# Star it. Now. >:-) *stares menacingly*\n",
    "\n",
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import gym\n",
    "import pybullet_envs\n",
    "\n",
    "from gym import wrappers as w\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "from typing import List, Any\n",
    "\n",
    "from numba import jit # I AM SPEEEEED (JIT-Compilation)\n",
    "import multiprocessing as mp # I AM SPEEEED Part 2 (Multiprocessing)\n",
    "\n",
    "from os.path import exists\n",
    "from os import mkdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-lodge",
   "metadata": {},
   "source": [
    "## 2.) **A simple MLP with the Hebbian Update Rule**\n",
    "As in RL, we start out by building a simple policy network. It is a fully connected Multi Layer Perceptron without bias.\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align:center\"><img src=\"../assets/hebbiannetwork.png\"></div>\n",
    "<br>\n",
    "\n",
    "The key differences is the way we update it's weights. Gradient descend, which is a global update rule and therefore not biologically accurate, is replaced by the hebbian update rule. \n",
    "\n",
    "$$\\Delta w_{ij} = \\eta_w \\cdot o_i o_j$$\n",
    "\n",
    "This rule updates the weights dynamically throughout an episode using some evolved term inspired by the hebbian update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nuclear-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HebbianNetwork(nn.Module):\n",
    "    \"A simple MLP without bias\"\n",
    "    def __init__(self, input_space, action_space):\n",
    "        super(HebbianNetwork, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(input_space, 128, bias=False)\n",
    "        self.l2 = nn.Linear(128, 64, bias=False)\n",
    "        self.l3 = nn.Linear(64, action_space, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        state = torch.as_tensor(x).float().detach()\n",
    "        \n",
    "        x1 = torch.tanh(self.l1(state))   \n",
    "        x2 = torch.tanh(self.l2(x1))\n",
    "        o = self.l3(x2)  \n",
    "         \n",
    "        return state, x1, x2, o\n",
    "    \n",
    "# NUMBA JIT-Compilation goes brrrrrrrrrrr\n",
    "@jit(nopython=True)\n",
    "def hebbian_update_rule(heb_coeffs, weights1_2, weights2_3, weights3_4, o0, o1, o2, o3):\n",
    "    \n",
    "        #print(o3.shape)\n",
    "       \n",
    "        heb_offset = 0\n",
    "        # Layer 1         \n",
    "        for i in range(weights1_2.shape[1]): \n",
    "            for j in range(weights1_2.shape[0]):  \n",
    "                idx = (weights1_2.shape[0]-1)*i + i + j\n",
    "                weights1_2[:,i][j] += heb_coeffs[idx][3] * ( heb_coeffs[idx][0] * o0[i] * o1[j]\n",
    "                                                           + heb_coeffs[idx][1] * o0[i] \n",
    "                                                           + heb_coeffs[idx][2]         * o1[j]  + heb_coeffs[idx][4]) + heb_coeffs[idx][5]\n",
    "\n",
    "        heb_offset += weights1_2.shape[1] * weights1_2.shape[0]\n",
    "        # Layer 2\n",
    "        for i in range(weights2_3.shape[1]): \n",
    "            for j in range(weights2_3.shape[0]):  \n",
    "                idx = heb_offset + (weights2_3.shape[0]-1)*i + i+j\n",
    "                weights2_3[:,i][j] += heb_coeffs[idx][3] * ( heb_coeffs[idx][0] * o1[i] * o2[j]\n",
    "                                                           + heb_coeffs[idx][1] * o1[i] \n",
    "                                                           + heb_coeffs[idx][2]         * o2[j]  + heb_coeffs[idx][4]) + heb_coeffs[idx][5]\n",
    "    \n",
    "        heb_offset += weights2_3.shape[1] * weights2_3.shape[0]\n",
    "        # Layer 3\n",
    "        for i in range(weights3_4.shape[1]): \n",
    "            for j in range(weights3_4.shape[0]):  \n",
    "                idx = heb_offset + (weights3_4.shape[0]-1)*i + i+j \n",
    "                weights3_4[:,i][j] += heb_coeffs[idx][3] * ( heb_coeffs[idx][0] * o2[i] * o3[j]\n",
    "                                                           + heb_coeffs[idx][1] * o2[i] \n",
    "                                                           + heb_coeffs[idx][2]         * o3[j]  + heb_coeffs[idx][4]) + heb_coeffs[idx][5]\n",
    "\n",
    "        return weights1_2, weights2_3, weights3_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-encounter",
   "metadata": {},
   "source": [
    "Since the simplified hebbian update rule \n",
    "\n",
    "$$\\Delta w_{ij} = \\eta_w \\cdot o_i o_j$$\n",
    "\n",
    "cannot be used in supervised learning tasks due to only beeing a local update rule, we need to extend this rule in order to use it.\n",
    "\n",
    "The paper introduces four evolveable parameters for the hebbian update rule:\n",
    "\n",
    "- correlation term $A_w$\n",
    "- presynaptic term $B_w$\n",
    "- postsynaptic term $C_w$\n",
    "- bias $D_w$\n",
    "\n",
    "Which leads to a modified update rule, where the coefficients $A_w$, $B_w$, $C_w$ define the update dynamics of the network weights\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align:center\"><img src=\"../assets/hebbianrule.png\"></div>\n",
    "<br>\n",
    "\n",
    "These coefficients can be evolved using a basic ES algorithm which maximizes a cummulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-salvation",
   "metadata": {},
   "source": [
    "## 3.) **The Task**\n",
    "<img src=\"../assets/ant.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-fundamental",
   "metadata": {},
   "source": [
    "## 4.) **Evolution strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "loaded-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_hebb(environment : str, init_weights:str, evolved_parameters: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the policy network using some evolved parameters and environment.\n",
    "    \"\"\"\n",
    "    def weights_init(m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            if init_weights == 'xa_uni':  \n",
    "                torch.nn.init.xavier_uniform(m.weight.data, 0.3)\n",
    "            elif init_weights == 'sparse':  \n",
    "                torch.nn.init.sparse_(m.weight.data, 0.8)\n",
    "            elif init_weights == 'uni':  \n",
    "                torch.nn.init.uniform_(m.weight.data, -0.1, 0.1)\n",
    "            elif init_weights == 'normal':  \n",
    "                torch.nn.init.normal_(m.weight.data, 0, 0.024)\n",
    "            elif init_weights == 'ka_uni':  \n",
    "                torch.nn.init.kaiming_uniform_(m.weight.data, 3)\n",
    "            elif init_weights == 'uni_big':\n",
    "                torch.nn.init.uniform_(m.weight.data, -1, 1)\n",
    "            elif init_weights == 'xa_uni_big':\n",
    "                torch.nn.init.xavier_uniform(m.weight.data)\n",
    "            elif init_weights == 'ones':\n",
    "                torch.nn.init.ones_(m.weight.data)\n",
    "            elif init_weights == 'zeros':\n",
    "                torch.nn.init.zeros_(m.weight.data)\n",
    "            elif init_weights == 'default':\n",
    "                pass\n",
    "            \n",
    "    # Unpack evolved parameters\n",
    "    hebb_coeffs = evolved_parameters\n",
    "\n",
    "    \n",
    "    # disable the autograd system\n",
    "    with torch.no_grad():\n",
    "                    \n",
    "        # Load environment\n",
    "        try:\n",
    "            env = gym.make(environment, verbose = 0)\n",
    "        except:\n",
    "            env = gym.make(environment)\n",
    "            \n",
    "        # env.render()  # render bullet envs\n",
    "\n",
    "        # get the input dimensions of the environment\n",
    "        input_dim = env.observation_space.shape[0]\n",
    "            \n",
    "        # Determine action space dimension\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        \n",
    "        # Initialize policy network\n",
    "        p = HebbianNetwork(input_dim, action_dim)          \n",
    "          \n",
    "        # Randomly sample initial weights from chosen distribution\n",
    "        p.apply(weights_init)\n",
    "        p = p.float()\n",
    "        \n",
    "        # Unpack network's weights\n",
    "        weights1_2, weights2_3, weights3_4 = list(p.parameters())\n",
    "            \n",
    "        # JIT\n",
    "        weights1_2 = weights1_2.detach().numpy()\n",
    "        weights2_3 = weights2_3.detach().numpy()\n",
    "        weights3_4 = weights3_4.detach().numpy()\n",
    "        \n",
    "        # reset the environment\n",
    "        observation = env.reset() \n",
    "\n",
    "        # Burnout phase for the bullet ant so it starts off from the floor\n",
    "        if environment == 'AntBulletEnv-v0':\n",
    "            action = np.zeros(8)\n",
    "            for _ in range(40):\n",
    "                __ = env.step(action)        \n",
    "        \n",
    "        # Normalize weights flag for non-bullet envs\n",
    "        normalised_weights = False if environment[-12:-6] == 'Bullet' else True\n",
    "\n",
    "\n",
    "        # Main loop\n",
    "        neg_count = 0 # count the amount of times we receive a negative reward\n",
    "        rew_ep = 0 # cummulative reward over an episode\n",
    "        t = 0 # timestep\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            # For obaservation âˆˆ gym.spaces.Discrete, we one-hot encode the observation\n",
    "            if isinstance(env.observation_space, Discrete): \n",
    "                observation = (observation == torch.arange(env.observation_space.n)).float()\n",
    "            \n",
    "            o0, o1, o2, o3 = p([observation])\n",
    "            \n",
    "            # JIT\n",
    "            o0 = o0.numpy().flatten()\n",
    "            o1 = o1.numpy().flatten()\n",
    "            o2 = o2.numpy().flatten()\n",
    "            \n",
    "            # preprocess the observation\n",
    "            o3 = torch.tanh(o3).numpy().flatten()\n",
    "            action = o3\n",
    "            \n",
    "            \n",
    "            # Environment simulation step\n",
    "            observation, reward, done, info = env.step(action)  \n",
    "            if environment == 'AntBulletEnv-v0': reward = env.unwrapped.rewards[1] # Distance walked\n",
    "            rew_ep += reward\n",
    "            \n",
    "            # env.render('human') # Gym envs\n",
    "                                       \n",
    "            # Early stopping conditions\n",
    "            if environment[-12:-6] == 'Bullet':\n",
    "                ## Special stopping condition for bullet envs\n",
    "                # always play 200 episodes\n",
    "                if t > 200:\n",
    "                    # after 200 episodes: count the amount of negative\n",
    "                    # reward we receive in a row\n",
    "                    neg_count = neg_count+1 if reward < 0.0 else 0\n",
    "                    \n",
    "                    # if we receive a negative reward 30 times in row, stop\n",
    "                    if (done or neg_count > 30):\n",
    "                        break\n",
    "            else:\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            t += 1\n",
    "            \n",
    "            #### Episodic/Intra-life hebbian update of the weights\n",
    "            weights1_2, weights2_3, weights3_4 = hebbian_update_rule(hebb_coeffs, weights1_2, weights2_3, weights3_4, o0, o1, o2, o3)\n",
    "            \n",
    "\n",
    "            # Normalise weights per layer\n",
    "            if normalised_weights == True:\n",
    "                (a, b, c) = (0, 1, 2) if not pixel_env else (2, 3, 4)\n",
    "                list(p.parameters())[a].data /= list(p.parameters())[a].__abs__().max()\n",
    "                list(p.parameters())[b].data /= list(p.parameters())[b].__abs__().max()\n",
    "                list(p.parameters())[c].data /= list(p.parameters())[c].__abs__().max()\n",
    "        \n",
    "        # close the environment\n",
    "        env.close()\n",
    "\n",
    "    return rew_ep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "strange-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_worker(arg):\n",
    "    get_reward_func,  env,  init_weights, coeffs = arg\n",
    "    \n",
    "    wp = np.array(coeffs)\n",
    "    decay = - 0.01 * np.mean(wp**2)\n",
    "    \n",
    "    r = get_reward_func(env,  init_weights, coeffs) + decay\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "american-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranks(x):\n",
    "    \"\"\"\n",
    "    Returns rank as a vector of len(x) with integers from 0 to len(x)\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1\n",
    "    ranks = np.empty(len(x), dtype=int)\n",
    "    ranks[x.argsort()] = np.arange(len(x))\n",
    "    return ranks\n",
    "\n",
    "def compute_centered_ranks(x):\n",
    "    \"\"\"\n",
    "    Maps x to [-0.5, 0.5] and returns the rank\n",
    "    \"\"\"\n",
    "    y = compute_ranks(x.ravel()).reshape(x.shape).astype(np.float32)\n",
    "    y /= (x.size - 1)\n",
    "    y -= .5\n",
    "    return y\n",
    "\n",
    "class EvolutionStrategies(object):\n",
    "    def __init__(self, environment, init_weights=\"uni\", population_size=100, sigma=0.1, learning_rate=0.2, decay=0.995, num_threads=-1, distribution = 'normal'):\n",
    "                             \n",
    "        self.environment = environment                         \n",
    "        self.init_weights = init_weights              \n",
    "        self.POPULATION_SIZE = population_size\n",
    "        self.SIGMA = sigma\n",
    "        self.learning_rate = learning_rate            \n",
    "        self.decay = decay\n",
    "        self.num_threads = mp.cpu_count() if num_threads == -1 else num_threads\n",
    "        print(f\"[INFO] Using upto {self.num_threads} threads.\")\n",
    "        self.update_factor = self.learning_rate / (self.POPULATION_SIZE * self.SIGMA)\n",
    "        self.distribution = distribution\n",
    "        \n",
    "        self.coefficients_per_synapse = 6\n",
    "        \n",
    "        # make the environment\n",
    "        env = gym.make(environment)\n",
    "\n",
    "        # get the input dimensions of the environment\n",
    "        input_dim = env.observation_space.shape[0]\n",
    "            \n",
    "        # Determine action space dimension\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        \n",
    "        # initialize the weights\n",
    "        plastic_weights = (128*input_dim) + (64*128) + (action_dim*64)\n",
    "        \n",
    "        if self.distribution == 'uniform': \n",
    "            self.coeffs = np.random.uniform(-1,1,(plastic_weights, self.coefficients_per_synapse)) \n",
    "        elif self.distribution == 'normal':\n",
    "            self.coeffs = torch.randn(plastic_weights, self.coefficients_per_synapse).detach().numpy().squeeze() \n",
    "        \n",
    "        # set the reward function\n",
    "        self.get_reward = fitness_hebb\n",
    "        \n",
    "    def _get_params_try(self, w, p):\n",
    "\n",
    "        param_try = []\n",
    "        for index, i in enumerate(p):\n",
    "            jittered = self.SIGMA * i\n",
    "            param_try.append(w[index] + jittered)\n",
    "        param_try = np.array(param_try).astype(np.float32)\n",
    "        \n",
    "        return param_try\n",
    "    \n",
    "    def get_coeffs(self):\n",
    "        return self.coeffs.astype(np.float32)\n",
    "    \n",
    "    def _get_population(self, coevolved_param = False): \n",
    "        \n",
    "    \n",
    "        # x_ = np.random.randn(int(self.POPULATION_SIZE/2), self.coeffs.shape[0], self.coeffs[0].shape[0])\n",
    "        # population = np.concatenate((x_,-1*x_)).astype(np.float32)\n",
    "        \n",
    "        population = []\n",
    "            \n",
    "        if coevolved_param == False:\n",
    "            for i in range( int(self.POPULATION_SIZE/2) ):\n",
    "                x = []\n",
    "                x2 = []\n",
    "                for w in self.coeffs:\n",
    "                    j = np.random.randn(*w.shape)             # j: (coefficients_per_synapse, 1) eg. (5,1)\n",
    "                    x.append(j)                                                   # x: (coefficients_per_synapse, number of synapses) eg. (92690, 5)\n",
    "                    x2.append(-j) \n",
    "                population.append(x)                                              # population : (population size, coefficients_per_synapse, number of synapses), eg. (10, 92690, 5)\n",
    "                population.append(x2)\n",
    "                \n",
    "        elif coevolved_param == True:\n",
    "            for i in range( int(self.POPULATION_SIZE/2) ):\n",
    "                x = []\n",
    "                x2 = []\n",
    "                for w in self.initial_weights_co:\n",
    "                    j = np.random.randn(*w.shape)\n",
    "                    x.append(j)                    \n",
    "                    x2.append(-j) \n",
    "\n",
    "                population.append(x)               \n",
    "                population.append(x2)\n",
    "                \n",
    "        return np.array(population).astype(np.float32)\n",
    "    \n",
    "    def _get_rewards(self, pool, population):\n",
    "        if pool is not None:\n",
    "\n",
    "            worker_args = []\n",
    "            for p in population:\n",
    "\n",
    "                heb_coeffs_try1 = []\n",
    "                for index, i in enumerate(p):\n",
    "                    jittered = self.SIGMA * i\n",
    "                    heb_coeffs_try1.append(self.coeffs[index] + jittered) \n",
    "                heb_coeffs_try = np.array(heb_coeffs_try1).astype(np.float32)\n",
    "\n",
    "                worker_args.append( (self.get_reward, self.environment,  self.init_weights,  heb_coeffs_try) )\n",
    "                \n",
    "            rewards = pool.map(agent_worker, worker_args)\n",
    "            \n",
    "        else:\n",
    "            rewards = []\n",
    "            for p in population:\n",
    "                heb_coeffs_try = np.array(self._get_params_try(self.coeffs, p))\n",
    "                rewards.append(self.get_reward(self.environment,  self.init_weights, heb_coeffs_try))\n",
    "        \n",
    "        rewards = np.array(rewards).astype(np.float32)\n",
    "        return rewards\n",
    "    \n",
    "    def _update_coeffs(self, rewards, population):\n",
    "        rewards = compute_centered_ranks(rewards)\n",
    "\n",
    "        std = rewards.std()\n",
    "        if std == 0:\n",
    "            raise ValueError('Variance should not be zero')\n",
    "                \n",
    "        rewards = (rewards - rewards.mean()) / std\n",
    "                \n",
    "        for index, c in enumerate(self.coeffs):\n",
    "            layer_population = np.array([p[index] for p in population])\n",
    "                      \n",
    "            self.update_factor = self.learning_rate / (self.POPULATION_SIZE * self.SIGMA)                \n",
    "            self.coeffs[index] = c + self.update_factor * np.dot(layer_population.T, rewards).T \n",
    "\n",
    "        if self.learning_rate > 0.001:\n",
    "            self.learning_rate *= self.decay\n",
    "\n",
    "        #Decay sigma\n",
    "        if self.SIGMA>0.01:\n",
    "            self.SIGMA *= 0.999 \n",
    "            \n",
    "            \n",
    "    def run(self, iterations, print_step=10, path='heb_coeffs'):                                                    \n",
    "        \n",
    "        id_ = str(int(time.time()))\n",
    "        if not exists(path + '/' + id_):\n",
    "            mkdir(path + '/' + id_)\n",
    "            \n",
    "        print('Run: ' + id_ + '\\n\\n........................................................................\\n')\n",
    "            \n",
    "        pool = mp.Pool(self.num_threads) if self.num_threads > 1 else None\n",
    "        \n",
    "        generations_rewards = []\n",
    "\n",
    "        for iteration in range(iterations):                                                                     # Algorithm 2. Salimans, 2017: https://arxiv.org/abs/1703.03864\n",
    "            population = self._get_population()                                                                 # Sample normal noise:         Step 5\n",
    "            rewards = self._get_rewards(pool, population)                                                       # Compute population fitness:  Step 6\n",
    "            self._update_coeffs(rewards, population)                                                            # Update coefficients:         Steps 8->12\n",
    "                \n",
    "                \n",
    "            # Print fitness and save Hebbian coefficients\n",
    "            if (iteration + 1) % print_step == 0:\n",
    "                rew_ = rewards.mean()\n",
    "                print('iter %4i | reward: %3i |  update_factor: %f  lr: %f | sum_coeffs: %i sum_abs_coeffs: %4i' % (iteration + 1, rew_ , self.update_factor, self.learning_rate, int(np.sum(self.coeffs)), int(np.sum(abs(self.coeffs)))), flush=True)\n",
    "                \n",
    "                if rew_ > 100:\n",
    "                    torch.save(self.get_coeffs(),  path + \"/\"+ id_ + '/HEBcoeffs__' + self.environment + \"__rew_\" + str(int(rew_)) + '__' + self.hebb_rule + \"__init_\" + str(self.init_weights) + \"__pop_\" + str(self.POPULATION_SIZE) + '__coeffs' + \"__{}.dat\".format(iteration))\n",
    "                generations_rewards.append(rew_)\n",
    "                np.save(path + \"/\"+ id_ + '/Fitness_values_' + id_ + '_' + self.environment + '.npy', np.array(generations_rewards))\n",
    "       \n",
    "        if pool is not None:\n",
    "            pool.close()\n",
    "            pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "arranged-encounter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using upto 8 threads.\n"
     ]
    }
   ],
   "source": [
    "es = EvolutionStrategies(\"AntBulletEnv-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "continued-deposit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1621328743\n",
      "\n",
      "........................................................................\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luanademi/anaconda3/envs/datascience/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/luanademi/anaconda3/envs/datascience/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/luanademi/anaconda3/envs/datascience/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/luanademi/anaconda3/envs/datascience/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/luanademi/anaconda3/envs/datascience/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/luanademi/anaconda3/envs/datascience/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/luanademi/anaconda3/envs/datascience/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/luanademi/anaconda3/envs/datascience/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    1 | reward:   0 |  update_factor: 0.020000  lr: 0.199000 | sum_coeffs: 336 sum_abs_coeffs: 59777\n",
      "iter    2 | reward:   5 |  update_factor: 0.019920  lr: 0.198005 | sum_coeffs: 406 sum_abs_coeffs: 61177\n",
      "iter    3 | reward:   7 |  update_factor: 0.019840  lr: 0.197015 | sum_coeffs: 337 sum_abs_coeffs: 62113\n",
      "iter    4 | reward:  16 |  update_factor: 0.019761  lr: 0.196030 | sum_coeffs: 382 sum_abs_coeffs: 63402\n",
      "iter    5 | reward:   7 |  update_factor: 0.019682  lr: 0.195050 | sum_coeffs: 389 sum_abs_coeffs: 64508\n",
      "iter    6 | reward:  13 |  update_factor: 0.019603  lr: 0.194075 | sum_coeffs: 330 sum_abs_coeffs: 65751\n",
      "iter    7 | reward:  19 |  update_factor: 0.019524  lr: 0.193104 | sum_coeffs: 331 sum_abs_coeffs: 66782\n",
      "iter    8 | reward:  18 |  update_factor: 0.019446  lr: 0.192139 | sum_coeffs: 334 sum_abs_coeffs: 67518\n",
      "iter    9 | reward:  14 |  update_factor: 0.019368  lr: 0.191178 | sum_coeffs: 432 sum_abs_coeffs: 68514\n",
      "iter   10 | reward:  15 |  update_factor: 0.019291  lr: 0.190222 | sum_coeffs: 360 sum_abs_coeffs: 69606\n",
      "iter   11 | reward:  12 |  update_factor: 0.019213  lr: 0.189271 | sum_coeffs: 380 sum_abs_coeffs: 70688\n",
      "iter   12 | reward:  17 |  update_factor: 0.019137  lr: 0.188325 | sum_coeffs: 472 sum_abs_coeffs: 71634\n",
      "iter   13 | reward:  17 |  update_factor: 0.019060  lr: 0.187383 | sum_coeffs: 513 sum_abs_coeffs: 72808\n",
      "iter   14 | reward:  18 |  update_factor: 0.018984  lr: 0.186446 | sum_coeffs: 497 sum_abs_coeffs: 73838\n",
      "iter   15 | reward:  20 |  update_factor: 0.018908  lr: 0.185514 | sum_coeffs: 431 sum_abs_coeffs: 74989\n",
      "iter   16 | reward:  20 |  update_factor: 0.018832  lr: 0.184586 | sum_coeffs: 452 sum_abs_coeffs: 75656\n",
      "iter   17 | reward:  24 |  update_factor: 0.018756  lr: 0.183663 | sum_coeffs: 459 sum_abs_coeffs: 76700\n",
      "iter   18 | reward:  24 |  update_factor: 0.018681  lr: 0.182745 | sum_coeffs: 511 sum_abs_coeffs: 77565\n",
      "iter   19 | reward:  24 |  update_factor: 0.018607  lr: 0.181831 | sum_coeffs: 414 sum_abs_coeffs: 78493\n",
      "iter   20 | reward:  16 |  update_factor: 0.018532  lr: 0.180922 | sum_coeffs: 408 sum_abs_coeffs: 79380\n",
      "iter   21 | reward:  23 |  update_factor: 0.018458  lr: 0.180017 | sum_coeffs: 467 sum_abs_coeffs: 80292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-19:\n",
      "Process ForkPoolWorker-23:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-24:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-17:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-21:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/queues.py\", line 356, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-134bc10c0e30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-56a8b2828942>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, iterations, print_step, path)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m                                                                     \u001b[0;31m# Algorithm 2. Salimans, 2017: https://arxiv.org/abs/1703.03864\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mpopulation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_population\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                                                 \u001b[0;31m# Sample normal noise:         Step 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m                                                       \u001b[0;31m# Compute population fitness:  Step 6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_coeffs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m                                                            \u001b[0;31m# Update coefficients:         Steps 8->12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-56a8b2828942>\u001b[0m in \u001b[0;36m_get_rewards\u001b[0;34m(self, pool, population)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mheb_coeffs_try1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mjittered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGMA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mheb_coeffs_try1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeffs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mjittered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mheb_coeffs_try\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheb_coeffs_try1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/queues.py\", line 355, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/luanademi/anaconda3/envs/datascience/lib/python3.8/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "es.run(100, print_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.make(\"AntBulletEnv-v0\").action_space.sample().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-patient",
   "metadata": {},
   "source": [
    "# References\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
